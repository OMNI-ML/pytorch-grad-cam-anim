
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Introduction &#8212; CAManim</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet">
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=4ec06e9971c5264fbd345897d5258098f11cc577" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94">
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=8bf782fb4ee92b3d3646425e50f299c4e1fd152d"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'CAManim-CRV2023/main';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../intro.html">

  
  
  
  
  
  
  

  
    <img src="https://omni.ohri.ca/wp-content/uploads/2020/07/image-20200714-152753-1dba07da-1024x85.jpg" class="logo__image only-light" alt="Logo image">
    <img src="https://omni.ohri.ca/wp-content/uploads/2020/07/image-20200714-152753-1dba07da-1024x85.jpg" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../animations_breast_model.html">
                        CAManim videos
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../animations_densenet_bear.html">
                        CAManim videos
                      </a>
                    </li>
                
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      
        <div class="navbar-end-item navbar-persistent--container">
          
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
        </div>
      
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>


  
  
    <div class="navbar-persistent--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
    </div>
  

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../animations_breast_model.html">
                        CAManim videos
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../animations_densenet_bear.html">
                        CAManim videos
                      </a>
                    </li>
                
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item">
  


<a class="navbar-brand logo" href="../intro.html">

  
  
  
  
  
  
  

  
    <img src="https://omni.ohri.ca/wp-content/uploads/2020/07/image-20200714-152753-1dba07da-1024x85.jpg" class="logo__image only-light" alt="Logo image">
    <img src="https://omni.ohri.ca/wp-content/uploads/2020/07/image-20200714-152753-1dba07da-1024x85.jpg" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    </div>
    <div class="sidebar-start-items__item">
<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to CAManim!
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../animations_breast_model.html">CAManim videos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../animations_densenet_bear.html">CAManim videos</a></li>
</ul>

    </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

  
  <div id="rtd-footer-container"></div>

      </div>
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        <label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" data-toggle="tooltip" data-placement="right" title="Toggle primary sidebar">
            <span class="fa-solid fa-bars"></span>
        </label>
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="btn btn-sm"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<div class="dropdown dropdown-repository-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="https://github.com/OMNI-ML/pytorch-grad-cam-anim" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">repository</span>
</a>
</a>
      
      <li><a href="https://github.com/OMNI-ML/pytorch-grad-cam-anim/issues/new?title=Issue%20on%20page%20%2FCAManim-CRV2023/main.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">open issue</span>
</a>
</a>
      
  </ul>
</div>



<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="../_sources/CAManim-CRV2023/main.md" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</a>
      
      <li>
<button onclick="printPdf(this)"
  class="btn btn-sm dropdown-item"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</a>
      
  </ul>
</div>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary" data-toggle="tooltip" data-placement="left" title="Toggle secondary sidebar">
            <span class="fa-solid fa-list"></span>
        </label>
    </div>
</div>
            </div>
            
            

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Introduction
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#related-work">
   Related Work
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#methodology">
   Methodology
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#individual-cam-formulation">
     Individual CAM Formulation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#end-to-end-layerwise-activation-maps">
     End-to-End Layerwise Activation Maps
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#global-vs-local-layer-normalization">
     Global- vs. Local-Layer Normalization
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-and-cam-specific-interpretation">
     Model- and CAM-Specific Interpretation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computational-complexity">
     Computational Complexity
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#quantitative-evaluation">
     Quantitative Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#results-discussion">
   Results &amp; Discussion
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pre-trained-models-and-datasets-sec-datamodel">
     Pre-trained Models and Datasets {#sec:datamodel}
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-study-end-to-end-bc-resnet50-visualization-for-malignant-tumour-prediction-sec-bcresnet">
     Case Study: End-to-End BC-ResNet50 Visualization for Malignant Tumour Prediction {#sec:bcresnet}
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-end-to-end-network-activation-maps-sec-activationmaps">
     Visualizing End-to-End Network Activation Maps {#sec:activationmaps}
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layer-type-visualization-issues-sec-visissues">
     Layer-Type Visualization Issues {#sec:visissues}
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ybroad-quantitative-evaluation-sec-ybroad">
     ybROAD Quantitative Evaluation {#sec:ybROAD}
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#acknowledgment-acknowledgment-unnumbered">
   Acknowledgment {#acknowledgment .unnumbered}
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>

            <article class="bd-article" role="main">
              
  <div class="topic abstract" role="doc-abstract">
<p class="topic-title">Abstract</p>
<p>Deep neaural networks have been widely adopted in numerous domains due
to their high performance and increasing accessibility to developers
and their application-specific end-user. Fundamental to image-based
applications is the development and refinement of Convolutional Neural
Networks (CNNs), which possess the ability to automatically extract
features from data. However, comprehending these complex models and
their learned representations, which typically comprise millions of
parameters and numerous layers, remains a challenge for both
developers and end-users. This challenge arises, in part, due to the
absence of interpretable and transparent tools to make sense of
black-box models. There exists a growing body of Explainable
Artificial Intelligence (XAI) literature, including a collection of
methods denoted as Class Activation Map (CAM), that seeks to demystify
what representations the model learns from the data, how it informs a
given prediction, and why it, at times, performs poorly in certain
tasks. This work proposes a novel XAI visualization method denoted
CAManim that seeks to simultaneously broaden and focus end-user
understanding of CNN predictions by animating the CAM-based network
activation maps through all layers, effectively depicting from
end-to-end how a model progressively arrives at the final layer
activation. Herein, we demonstrate that CAManim works with any
CAM-based method and various CNN architectures. Beyond the qualitative
model assessment focus of this work, we additionally propose a novel
quantitative assessment that expands upon the Remove and Debias (ROAD)
metric that considers pairs the qualitative end-to-end network visual
explanations assessment with our novel quantitative
“yellow-brick-ROAD” assessment (ybROAD). This builds upon prior
research to address the increasing demand for interpretable, robust,
and transparent model assessment methodology ultimately generating
increasing trust in a given end-user upon a model’s predictions.
</p>
</div>
<div class="highlight-IEEEkeywords notranslate"><div class="highlight"><pre><span></span>explainable artificial intelligence; convolutional neural networks;
classification
</pre></div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h1>
<p>The popularization of deep learning in numerous domains of research has
led to the rapid adoption of these methodologies in disparate fields of
scientific research. Convolutional Neural Networks (CNNs) are a class of
deep learning models that use convolutions to extract image features,
achieving high performance in numerous computer vision applications.
However, due to the intrinsic network structure and the complexity of
features leveraged for model predictions, CNNs are, consequently, often
labeled as uninterpretable or ‘black-box; models. Interpretability is
crucial for applications in high-criticality fields such as medicine,
where model decisions have the potential to cause excessive harm if
incorrect. In order to be deployed, models must be trustworthy both in
their class predictions and in the features used to make those
predictions. Therefore, there is a definitive impetus to develop
trustworthy explanations of model decisions.</p>
<p>There have been numerous methods proposed to improve the
interpretability of CNNs. Zeiler and Fergus initially investigated
network interpretability by using a deconvolutional network to identify
pixels activated in CNN feature maps [&#64;zeiler2014visualizing]. Next,
gradient-based methods were used to develop saliency maps indicating
important image regions based on desired output class
[&#64;simonyan2013deep; &#64;springenberg2014striving; &#64;sundararajan2017axiomatic].
Class Activation Maps are a group of methods that linearly combine
weighted feature activation maps from a given CNN layer
[&#64;zhou16; &#64;gradcam; &#64;gradcampp; &#64;Fu20; &#64;Gildenblat21; &#64;Draelos20; &#64;Jiang21; &#64;Wang20; &#64;Desai20; &#64;eigencam].
Typically, only the final layer(s) are visualised to confer
trustworthiness and describe what image features are used for model
predictions. However, this provides little detail on the learning
process of the model. In addition, selecting the correct final layer to
visualize from each CNN model is not straightforward and is often done
arbitrarily.</p>
<p>To better interpret how a given model considers a given image through
each of its layers, we can individually visualize the model’s layer-wise
activation map. In a natural extension of this idea, these layer-wise
activation maps can be combined as individual frames of a video
animating the end-to-end network activation maps; a method we propose in
this article and denote CAManim. We develop local and global
normalization to understand learned network features on a layer-wise and
network-wise scale. We experiment and quantify layer-wise performance of
CAManim with numerous CNN models and CAM variations to show performance
in a variety of experimental conditions, including medical applications.</p>
<p>Our contributions are as follows:</p>
<ul class="simple">
<li><p>We propose CAManim, a novel visualization method that creates
activation maps for each layer in a given CNN. CAManim can be
applied to any existing CAM or CNN.</p></li>
<li><p>We introduce local and global normalization to understand important
learned features on a layer-wise and network-wise level.</p></li>
<li><p>We perform extensive experimentation to determine the computational
time and complexity required to run CAManim.</p></li>
<li><p>We demonstrate the usefulness of CAManim across multiple CAM
variations and CNN models, and in high criticality fields.</p></li>
<li><p>We quantitatively evaluate the performance of each CAM generated per
model layer with a metric termed ybROAD, improving the understanding
of how CNNs learn. This is further extended to selecting the most
accurate feature map representation from all possible layers of a
CNN.</p></li>
</ul>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="related-work">
<h1>Related Work<a class="headerlink" href="#related-work" title="Permalink to this heading">#</a></h1>
<p>The topic of explainable and trustworthy AI has been researched
extensively. Lipton <em>et al.</em> [&#64;lipton2018mythos] emphasized the need for
interpretable and trustworthy networks. Ribeiro <em>et al.</em>
[&#64;ribeiro2016should] conducted studies to assess if humans can place
trust in a classifier. Computationally, numerous methods have
investigated the improvement of CNN interpretation. In this section, we
provide an overview of proposed methods and how CAManim addresses a gap
in the current literature.</p>
<p><em>Earliest Explainable AI Studies:</em> One of the earliest efforts to
interpret CNNs was made by Zeiler and Fergus[&#64;zeiler2014visualizing]. In
this study, feature maps from convolutional layers are used as input to
a deconvolutional network to identify activated pixels in the original
image space. Simonyan <em>et al.</em> [&#64;simonyan2013deep] approached network
explainability in two ways. First, they proposed class models, which are
images generated through gradient ascent that maximize the score for a
given class. Next, they produced class-specific saliency maps,
calculated using the gradient of the input image with respect to a given
class.</p>
<p><em>Guided Backpropagation and Gradient-Based Methods:</em> Springenberg <em>et
al.</em> [&#64;springenberg2014striving] extended Simonyan’s work to Guided
Backpropagation, which excludes all negative gradients to improve
quality of saliency maps. These works are compared in
[&#64;mahendran2016salient]. Despite calculating gradients with respect to
individual classes, Selvaraju <em>et al.</em> showed that the visualizations
produced by Guided Backpropagation are not class-discriminative (<em>i.e.</em>
there is little difference between images generated using different
class nodes)[&#64;gradcam]. Sundarajan <em>et al.</em> [&#64;sundararajan2017axiomatic]
proposed integrated gradients, calculated through the integral of the
gradient between a given image and baseline, to satisfy axioms of
sensitivity and implementation invariance. FullGrad is another
gradient-based method that is non-discriminative and uses the gradients
of bias layers to produce saliency maps [&#64;Srinivas19].</p>
<p><em>Gradient-Free Methods:</em> While gradient-based methods are quite popular
in the field of explainable AI, some studies argue that these methods
produce noisy visualizations due to gradient saturation
[&#64;adebayo2018sanity; &#64;kindermans2019reliability]. For this reason,
gradient-free methods have been investigated by a number of studies.
Zhou <em>et al.</em> [&#64;zhou2014object] identified <em>K</em> images with the highest
activation at a given neuron in a convolutional layer and occludes
patches of each image to determine the object detected by the neuron.
Morcos <em>et al.</em> [&#64;morcos2018importance] used an ablation analysis to
remove individual neurons or feature maps from a CNN and quantify the
effect on network performance. This study demonstrated that neurons with
high class selectivity (<em>i.e.</em> highly activated for a single class) may
indicate poor network generalizability. Zhou <em>et al.</em>
[&#64;zhou2018revisiting] extended this work to show that ablating neurons
with high class selectivity may cause large differences in individual
class performance.</p>
<p><em>Class Activation Maps:</em> A popular class of CNN visualizations are Class
Activation Maps (CAMs), which produce explainable visualizations through
a linearly weighted sum of feature maps at a given CNN layer [&#64;zhou16].
The original CAM was proposed for a specific CNN model, consisting of
convolutional, global average pooling, and dense layers at the end of
the network. The dense layer weights were used to determine the weighted
importance of individual feature maps. However, this required a specific
CNN architecture and was not applicable to numerous high-performing
models. This led to the development of CNN model-agnostic CAM methods.</p>
<p>Gradient-based methods were the first variation of the original CAM
[&#64;gradcam; &#64;gradcampp; &#64;Fu20; &#64;Gildenblat21; &#64;Draelos20; &#64;Jiang21] .
These methods determine importance weights through calculating averaged
or elementwise gradients of the output of a class with respect to the
feature maps at the desired layer. As discussed previously, gradient
methods may produce noisy visualizations due to gradient saturation
[&#64;adebayo2018sanity; &#64;kindermans2019reliability; &#64;Wang20; &#64;Desai20; &#64;eigencam];
as a result, perturbation CAM methods have been proposed
[&#64;Wang20; &#64;Desai20] . In this case, importance weights are calculated by
perturbing the original input image by the feature maps A and measuring
the change in prediction score. In addition, non-discriminative
approaches have been investigated to eliminate the reliance of
class-discriminative methods on correct class predictions. EigenCAM
produces its CAM visualization using the principal components of the
activations maps at the desired layer [&#64;eigencam].</p>
<p>While most studies have developed saliency map and/or CAM formulations
for a single layer, LayerCAM demonstrated how aggregating feature maps
from multiple layers can refine the final CAM visualization to include
more fine-detailed information [&#64;Jiang21]. Gildenblat extended this idea
across existing multiple CAM and saliency map methods [&#64;Gildenblat21].
However, to the best of our knowledge, our study is the first to save
individual feature maps generated from every CNN layer and combine them
into an end-to-end network explanation.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="methodology">
<h1>Methodology<a class="headerlink" href="#methodology" title="Permalink to this heading">#</a></h1>
<p>In this section, we first recall the general formulation for Class
Activation Maps and outline notation preliminaries. Next, we explain the
generation of CAManim using CAMs from each layer of a CNN, depicted in
Figure <span class="xref myst">[fig:overview]</span>{reference-type=”ref”
reference=”fig:overview”}. The concepts of global and local
normalization are introduced, and the computational complexity of
CAManim is described. Lastly, we define the quantitative performance
metric for individual CAM visualizations, and propose ybROAD for
analyzing end-to-end layerwise CAManim.</p>
<section id="individual-cam-formulation">
<h2>Individual CAM Formulation<a class="headerlink" href="#individual-cam-formulation" title="Permalink to this heading">#</a></h2>
<p>The general formulation for any CAM method consists of taking a linearly
weighted sum of feature maps and is written as follows:</p>
<div class="math notranslate nohighlight">
\[\label{eqn:cam-form}
L^c_{CAM(A^l)} = \sum\limits_{k}(\alpha_k^cA_k^l),  \textit{where }  A^l= f ^l(x)\]</div>
<p>For a given input image <em>x</em> and CNN model <em>f</em>, a CAM visualization <em>L</em>
can be generated through the weighted <span class="math notranslate nohighlight">\(\alpha\)</span> summation of <em>k</em>
activation feature maps <em>A</em> at layer <em>l</em>. Class discriminative CAM
methods further define <em>L</em> per predicted class <em>c</em>. To exclude negative
activations, most CAM formulations are followed by a ReLU operation.</p>
</section>
<section id="end-to-end-layerwise-activation-maps">
<h2>End-to-End Layerwise Activation Maps<a class="headerlink" href="#end-to-end-layerwise-activation-maps" title="Permalink to this heading">#</a></h2>
<p>To formulate CAManim, CAM visualizations are first generated for every
differentiable layer <em>l</em> within a given CNN with a total number of
layers <em>N</em>:</p>
<div class="math notranslate nohighlight">
\[\label{eqn:cam-form}
L^c_{CAManim} = L^c_{CAM(A^{l=0})} ... L^c_{CAM(A^{l=N})}\]</div>
<p>Each CAM visualization is subsequently saved as a PNG image <em>I</em> and
concatenated together to create the final CAManim video, as depicted
below:</p>
<div class="math notranslate nohighlight">
\[\label{eqn:cam-form}
CAManim = \mathop{\mathrm{\scalerel*{\Vert}{\sum}}}_{l}^{N}I_{L^c_{CAM(A^{l})}}\]</div>
</section>
<section id="global-vs-local-layer-normalization">
<h2>Global- vs. Local-Layer Normalization<a class="headerlink" href="#global-vs-local-layer-normalization" title="Permalink to this heading">#</a></h2>
<p>CAManim can visualize the CAM activations of each layer using two
different types of normalization. Global normalization is performed
using the minimum and maximum activation value across all activations
generated, which is useful for visualizing which layer has the strongest
activation for a given class and provides network-level information.
Local normalization uses the minimum and maximum values of the
activations of each specific layer. Local normalization displays the
strongest activation of each individual layer and therefore provides
layer-wise information.</p>
<p>Figure <span class="xref myst">1</span>{reference-type=”ref”
reference=”fig:normalization”} shows the difference between global and
local normalization for the first denseblock of DenseNet161
<code class="docutils literal notranslate"><span class="pre">\cite{}</span></code>{=latex}. The global normalization (right) displays an
attenuated version of the local normalization (left). This example
demonstrates that the layer-wise information detecting learning small
features, whereas the network-wise information shows that the
activations of this layer are much smaller than other layers within
DenseNet161.</p>
<figure id="fig:normalization">
<img src="figures/Bear_Normalization.png" style="width:40.0%" />
<figcaption>Difference between local and global normalization for the
feature map generated from layer features.denseblock1 in
DenseNet161.</figcaption>
</figure>
</section>
<section id="model-and-cam-specific-interpretation">
<h2>Model- and CAM-Specific Interpretation<a class="headerlink" href="#model-and-cam-specific-interpretation" title="Permalink to this heading">#</a></h2>
</section>
<section id="computational-complexity">
<h2>Computational Complexity<a class="headerlink" href="#computational-complexity" title="Permalink to this heading">#</a></h2>
<figure id="fig:densenet-params">
<embed src="figures/densenet161_num-params.pdf" style="width:50.0%" />
<figcaption>Layer-wise Depiction of DenseNet161 parameters.</figcaption>
</figure>
</section>
<section id="quantitative-evaluation">
<h2>Quantitative Evaluation<a class="headerlink" href="#quantitative-evaluation" title="Permalink to this heading">#</a></h2>
<p>To quantitatively evaluate the performance of each CAM visualization and
demonstrate the information gained through deeper layers in a CNN, we
calculate the Remove and Debias (ROAD) score [&#64;Rong22]. This metric has
superior computational efficiency and prevents data leakage found with
other CAM performance metrics [&#64;Rong22]. ROAD perturbs images through
noisy linear imputations, blurring regions of the image based on
neighbouring pixel values. The confidence increase (or decrease) <em>C</em> in
classification score using the perturbed image with the least relevant
pixels <em>LRP</em> (or most relevant pixels <em>MRP</em>) is then used to evaluate
the accuracy of a CAM visualization. Since the percentage of pixels
perturbed affects the ROAD performance, we evaluate ROAD at <em>p =</em> 20%,
40%, 60% and 80% pixel perturbation thresholds. As proposed by
Gildenblat [&#64;Gildenblat21], we combine the least relevant pixel and most
relevant pixel scores for our final metric:</p>
<div class="math notranslate nohighlight">
\[ROAD(L^c_{CAM(A^{l})}) = \sum\limits_{p} \frac{(C^p_{L_{LRP}} – C^p_{L_{MRP}})}{2}\]</div>
<p>A ROAD score is calculated for each CAM generated. Therefore, for <em>N</em>
differentiable layers in a CNN, there will be <em>N</em> ROAD scores calculated
for CAManim. We denote this series of ROAD values as the ‘yellow brick
ROAD’, or ybROAD for short.</p>
<div class="math notranslate nohighlight">
\[\label{eqn:cam-form}
ybROAD = \mathop{\mathrm{\scalerel*{\Vert}{\sum}}}_{l}^{N}ROAD(L^c_{CAM(A^{l})})\]</div>
<p>The ybROAD metric can be used to analyze performance of an experiment
with given class, image, and CNN model over all layers of the network.
In this study, we identify the CNN layer that identifies features with
the largest impact on model performance through ybROAD_max. The
ybROAD_mean score is also calculated to summarize the overall ROAD
performance of a model.</p>
<div class="highlight-figure* notranslate"><div class="highlight"><pre><span></span>![image](figures/conceptual-overview.pdf){width=&quot;\\textwidth&quot;}
</pre></div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="results-discussion">
<h1>Results &amp; Discussion<a class="headerlink" href="#results-discussion" title="Permalink to this heading">#</a></h1>
<p>In this section, we first define the pre-trained models and datasets
used to evaluate CAManim in Section
<span class="xref myst">4.1</span>{reference-type=”ref” reference=”sec:datamodel”}.
Next, we demonstrate CAManim in high criticality fields using a ResNet50
model fine-tuned to perform breast cancer classification in Section
<span class="xref myst">4.2</span>{reference-type=”ref” reference=”sec:bcresnet”}. We
then show example visualizations from CAManim for 10 different CAM
variations in Section <span class="xref myst">4.3</span>{reference-type=”ref”
reference=”sec:activationmaps”} and discuss abnormal visualizations in
Section <span class="xref myst">4.4</span>{reference-type=”ref”
reference=”sec:visissues”}. Lastly, we discuss the ybROAD performance of
CAManim in Section <span class="xref myst">4.5</span>{reference-type=”ref”
reference=”sec:ybROAD”}.</p>
<section id="pre-trained-models-and-datasets-sec-datamodel">
<h2>Pre-trained Models and Datasets {#sec:datamodel}<a class="headerlink" href="#pre-trained-models-and-datasets-sec-datamodel" title="Permalink to this heading">#</a></h2>
<p>To evaluate CAManim, we use models from TorchVision pre-trained on the
2012 ImageNet-1K dataset. Specifically, results are shown for AlexNet
<code class="docutils literal notranslate"><span class="pre">\cite{}</span></code>{=latex}, ConvNext <code class="docutils literal notranslate"><span class="pre">\cite{}</span></code>{=latex}, DenseNet161
<code class="docutils literal notranslate"><span class="pre">\cite{}</span></code>{=latex}, EfficientNet-b7 <code class="docutils literal notranslate"><span class="pre">\cite{}</span></code>{=latex}, MaxViT-t
<code class="docutils literal notranslate"><span class="pre">\cite{}</span></code>{=latex}, and SqueezeNet <code class="docutils literal notranslate"><span class="pre">\cite{}</span></code>{=latex}. The CAManim videos
for an additional 14 models can be found here:
<a class="reference external" href="https://omni-ml.github.io/pytorch-grad-cam-anim/">https://omni-ml.github.io/pytorch-grad-cam-anim/</a>. All results in this
study (apart from the high criticality evaluation) are based on a
popular image used in CAM evaluations, preprocessed by resizing to 224 x
224 and normalizing the final image.</p>
<p>Next, we demonstrate the utility of CAManim in high criticality fields.
Specifically, we take a ResNet50 model pre-trained on the ImageNet
dataset, and fine-tune the model using the Kaggle breast ultrasound data
to classify malignant vs normal images <code class="docutils literal notranslate"><span class="pre">\cite{}</span></code>{=latex}. For
simplification, we call this network BC-ResNet50 (Breast Cancer
-ResNet50). This dataset consists of 133 normal images and 210 malignant
images, which are split into a 80-10-10% train-validation-test split.
Images are preprocessed to a size of 224 x 224 and augmentations are
applied to the training set. Preprocessing and training steps are
selected based on MONAI recommendations <code class="docutils literal notranslate"><span class="pre">\cite{}</span></code>{=latex}. After
fine-tuning the network, CAManim is run with an example test image of
the malignant class to understand how the CNN makes a correct prediction
of cancer.</p>
</section>
<section id="case-study-end-to-end-bc-resnet50-visualization-for-malignant-tumour-prediction-sec-bcresnet">
<h2>Case Study: End-to-End BC-ResNet50 Visualization for Malignant Tumour Prediction {#sec:bcresnet}<a class="headerlink" href="#case-study-end-to-end-bc-resnet50-visualization-for-malignant-tumour-prediction-sec-bcresnet" title="Permalink to this heading">#</a></h2>
<p>Figure <span class="xref myst">[fig:mednet-viz]</span>{reference-type=”ref”
reference=”fig:mednet-viz”} illustrates the layerwise activations that
BC-ResNet50 considers when determining the ‘malignant’ tumour.</p>
<div class="highlight-figure* notranslate"><div class="highlight"><pre><span></span>![image](figures/mednet-10percentile_final.pdf){width=&quot;\\textwidth&quot;}
</pre></div>
</div>
</section>
<section id="visualizing-end-to-end-network-activation-maps-sec-activationmaps">
<h2>Visualizing End-to-End Network Activation Maps {#sec:activationmaps}<a class="headerlink" href="#visualizing-end-to-end-network-activation-maps-sec-activationmaps" title="Permalink to this heading">#</a></h2>
<p>We demonstrate the performance of CAManim on 10 different CAM methods,
including seven gradient-based methods (EigenGradCAM, GradCAM,
GradCAMElementWise, GradCAM++, HiResCAm, LayerCAM, and XGradCAM), two
perturbation methods (AblationCAM and ScoreCAM), a principal components
method (EigenCAM), and RandomCAM. RandomCAM generates random feature
activation maps from a uniform distribution between -1 and 1.</p>
<div class="highlight-figure* notranslate"><div class="highlight"><pre><span></span>![image](figures/bear_one-model-all-cams_final.pdf){width=&quot;\\textwidth&quot;}
</pre></div>
</div>
<figure id="fig:my_label">
<embed src="figures/bear_one-cam-all-models_final.pdf"
style="width:50.0%" />
<figcaption>Initial, middle, and final activation maps applying a single
CAM, HiResCAM, to various model architechtures.</figcaption>
</figure>
</section>
<section id="layer-type-visualization-issues-sec-visissues">
<h2>Layer-Type Visualization Issues {#sec:visissues}<a class="headerlink" href="#layer-type-visualization-issues-sec-visissues" title="Permalink to this heading">#</a></h2>
<p>Certain differentiable layers may produce unanticipated CAM
visualizations, as depicted in Figure
<span class="xref myst">4</span>{reference-type=”ref” reference=”fig:bad_layers”}.
In these layers, images are compressed to 1-dimensional (1D)
representation, preventing valid 2D features from being discovered
through CAM formulations. Instead, individual neurons that are highly
activated show up as vertical or horizontal lines over the image. While
these images are not informative, they are not incorrect; they simply
depict visualizations of 1D layers.</p>
<figure id="fig:bad_layers">
<img src="figures/bad_layers.png" style="width:40.0%" />
<figcaption>Visualization of CAManim for fully connected and average
pooling layers.</figcaption>
</figure>
</section>
<section id="ybroad-quantitative-evaluation-sec-ybroad">
<h2>ybROAD Quantitative Evaluation {#sec:ybROAD}<a class="headerlink" href="#ybroad-quantitative-evaluation-sec-ybroad" title="Permalink to this heading">#</a></h2>
<p>Figure X(YBROAD) displays the ybROAD for X trials of generating CAManim
for the bear image using ResNet152. Initially, the individual-layer ROAD
performance is very high (<span class="math notranslate nohighlight">\(\sim\)</span><code class="docutils literal notranslate"><span class="pre">&lt;!--</span> <span class="pre">--&gt;</span></code>{=html}0.45). At this point,
the CNN layer is activating many small regions throughout the image;
when each of these areas is perturbed, it is difficult to correctly
classify the image, and the ROAD score increases. As the network starts
learning larger features, less of the bear image is perturbed, and the
ROAD score decreases. Towards the end of the network, the ROAD score
increases again as the small activations are combined together to cover
the entire bear. This demonstrates how the ybROAD score can provide
additional information on how the network is learning.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="conclusion">
<h1>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h1>
<p>The conclusion goes here. this is more of the conclusion</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="acknowledgment-acknowledgment-unnumbered">
<h1>Acknowledgment {#acknowledgment .unnumbered}<a class="headerlink" href="#acknowledgment-acknowledgment-unnumbered" title="Permalink to this heading">#</a></h1>
<p>The authors would like to thank… more thanks here</p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./CAManim-CRV2023"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

            </article>
            

            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Introduction
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#related-work">
   Related Work
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#methodology">
   Methodology
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#individual-cam-formulation">
     Individual CAM Formulation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#end-to-end-layerwise-activation-maps">
     End-to-End Layerwise Activation Maps
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#global-vs-local-layer-normalization">
     Global- vs. Local-Layer Normalization
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-and-cam-specific-interpretation">
     Model- and CAM-Specific Interpretation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computational-complexity">
     Computational Complexity
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#quantitative-evaluation">
     Quantitative Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#results-discussion">
   Results &amp; Discussion
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pre-trained-models-and-datasets-sec-datamodel">
     Pre-trained Models and Datasets {#sec:datamodel}
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-study-end-to-end-bc-resnet50-visualization-for-malignant-tumour-prediction-sec-bcresnet">
     Case Study: End-to-End BC-ResNet50 Visualization for Malignant Tumour Prediction {#sec:bcresnet}
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-end-to-end-network-activation-maps-sec-activationmaps">
     Visualizing End-to-End Network Activation Maps {#sec:activationmaps}
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layer-type-visualization-issues-sec-visissues">
     Layer-Type Visualization Issues {#sec:visissues}
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ybroad-quantitative-evaluation-sec-ybroad">
     ybROAD Quantitative Evaluation {#sec:ybROAD}
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#acknowledgment-acknowledgment-unnumbered">
   Acknowledgment {#acknowledgment .unnumbered}
  </a>
 </li>
</ul>

</nav>
</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Olivier Miguel
</p>

  </div>
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2022.<br>

</p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
Last updated on None.<br>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </div>
        </footer>
        

      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>

  </body>
</html>