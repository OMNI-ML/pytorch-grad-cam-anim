{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Define Google Drive location"
      ],
      "metadata": {
        "id": "pqygC7p6XR2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "drive.mount(\"/content/gdrive\")\n",
        "drive_path = '/content/gdrive/MyDrive'\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "save_root = Path(drive_path) / \"starCAManim\""
      ],
      "metadata": {
        "id": "pWjAEb55XR2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_xxaeFJNmLF"
      },
      "source": [
        "Copyright (c) MONAI Consortium  \n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
        "you may not use this file except in compliance with the License.  \n",
        "You may obtain a copy of the License at  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
        "Unless required by applicable law or agreed to in writing, software  \n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
        "See the License for the specific language governing permissions and  \n",
        "limitations under the License.\n",
        "\n",
        "# Medical Image Classification Tutorial with the MedNIST Dataset\n",
        "\n",
        "In this tutorial, we introduce an end-to-end training and evaluation example based on the MedNIST dataset.\n",
        "\n",
        "We'll go through the following steps:\n",
        "* Create a dataset for training and testing\n",
        "* Use MONAI transforms to pre-process data\n",
        "* Use the DenseNet from MONAI for classification\n",
        "* Train the model with a PyTorch program\n",
        "* Evaluate on test dataset\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/2d_classification/mednist_tutorial.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkg70Y8oNmLH"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PniqWSEkNmLH"
      },
      "outputs": [],
      "source": [
        "!python -c \"import monai\" || pip install -q \"monai-weekly[pillow, tqdm]\"\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09nJOoIpNmLI"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "VWyQnB-eNmLI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from monai.apps import download_and_extract\n",
        "from monai.config import print_config\n",
        "from monai.data import decollate_batch, DataLoader\n",
        "from monai.metrics import ROCAUCMetric\n",
        "from monai.networks.nets import DenseNet121, ResNet\n",
        "from monai.transforms import (\n",
        "    Activations,\n",
        "    EnsureChannelFirst,\n",
        "    AsDiscrete,\n",
        "    Compose,\n",
        "    LoadImage,\n",
        "    RandFlip,\n",
        "    RandRotate,\n",
        "    RandZoom,\n",
        "    ScaleIntensity,\n",
        ")\n",
        "from monai.utils import set_determinism\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRbmgmF_NmLI"
      },
      "source": [
        "## Setup data directory\n",
        "\n",
        "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
        "This allows you to save results and reuse downloads.  \n",
        "If not specified a temporary directory will be used."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"MONAI_DATA_DIRECTORY\"] = \"/content/\""
      ],
      "metadata": {
        "id": "d0qmFoDXexlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ZOuOF17TNmLI"
      },
      "outputs": [],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0Yp0HuFNmLJ"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "The MedNIST dataset was gathered from several sets from [TCIA](https://wiki.cancerimagingarchive.net/display/Public/Data+Usage+Policies+and+Restrictions),\n",
        "[the RSNA Bone Age Challenge](http://rsnachallenges.cloudapp.net/competitions/4),\n",
        "and [the NIH Chest X-ray dataset](https://cloud.google.com/healthcare/docs/resources/public-datasets/nih-chest).\n",
        "\n",
        "The dataset is kindly made available by [Dr. Bradley J. Erickson M.D., Ph.D.](https://www.mayo.edu/research/labs/radiology-informatics/overview) (Department of Radiology, Mayo Clinic)\n",
        "under the Creative Commons [CC BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/).\n",
        "\n",
        "If you use the MedNIST dataset, please acknowledge the source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "P_Fn4qO_NmLJ"
      },
      "outputs": [],
      "source": [
        "resource = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/MedNIST.tar.gz\"\n",
        "md5 = \"0bc7306e7427e00ad1c5526a6677552d\"\n",
        "\n",
        "compressed_file = os.path.join(root_dir, \"MedNIST.tar.gz\")\n",
        "data_dir = os.path.join(root_dir, \"MedNIST\")\n",
        "if not os.path.exists(data_dir):\n",
        "    download_and_extract(resource, compressed_file, root_dir, md5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWclHVUFNmLJ"
      },
      "source": [
        "## Set deterministic training for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-GiBZoONmLJ"
      },
      "outputs": [],
      "source": [
        "set_determinism(seed=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Uk5ZZVnNmLJ"
      },
      "source": [
        "## Read image filenames from the dataset folders\n",
        "\n",
        "First of all, check the dataset files and show some statistics.  \n",
        "There are 6 folders in the dataset: Hand, AbdomenCT, CXR, ChestCT, BreastMRI, HeadCT,  \n",
        "which should be used as the labels to train our classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "nz6AAp1gNmLK"
      },
      "outputs": [],
      "source": [
        "class_names = sorted(x for x in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, x)))\n",
        "num_class = len(class_names)\n",
        "image_files = [\n",
        "    [os.path.join(data_dir, class_names[i], x) for x in os.listdir(os.path.join(data_dir, class_names[i]))]\n",
        "    for i in range(num_class)\n",
        "]\n",
        "num_each = [len(image_files[i]) for i in range(num_class)]\n",
        "image_files_list = []\n",
        "image_class = []\n",
        "for i in range(num_class):\n",
        "    image_files_list.extend(image_files[i])\n",
        "    image_class.extend([i] * num_each[i])\n",
        "num_total = len(image_class)\n",
        "image_width, image_height = PIL.Image.open(image_files_list[0]).size\n",
        "\n",
        "print(f\"Total image count: {num_total}\")\n",
        "print(f\"Image dimensions: {image_width} x {image_height}\")\n",
        "print(f\"Label names: {class_names}\")\n",
        "print(f\"Label counts: {num_each}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akHYCsUWNmLK"
      },
      "source": [
        "## Randomly pick images from the dataset to visualize and check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGay9Iq1NmLK"
      },
      "outputs": [],
      "source": [
        "plt.subplots(3, 3, figsize=(8, 8))\n",
        "for i, k in enumerate(np.random.randint(num_total, size=9)):\n",
        "    print(image_files_list[k])\n",
        "    im = PIL.Image.open(image_files_list[k])\n",
        "    arr = np.array(im)\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.xlabel(class_names[image_class[k]])\n",
        "    plt.imshow(arr, cmap=\"gray\", vmin=0, vmax=255)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWa0aGKoNmLK"
      },
      "source": [
        "## Prepare training, validation and test data lists\n",
        "\n",
        "Randomly select 10% of the dataset as validation and 10% as test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "YaNJJQx5NmLK"
      },
      "outputs": [],
      "source": [
        "val_frac = 0.1\n",
        "test_frac = 0.1\n",
        "length = len(image_files_list)\n",
        "indices = np.arange(length)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "test_split = int(test_frac * length)\n",
        "val_split = int(val_frac * length) + test_split\n",
        "test_indices = indices[:test_split]\n",
        "val_indices = indices[test_split:val_split]\n",
        "train_indices = indices[val_split:]\n",
        "\n",
        "train_x = [image_files_list[i] for i in train_indices]\n",
        "train_y = [image_class[i] for i in train_indices]\n",
        "val_x = [image_files_list[i] for i in val_indices]\n",
        "val_y = [image_class[i] for i in val_indices]\n",
        "test_x = [image_files_list[i] for i in test_indices]\n",
        "test_y = [image_class[i] for i in test_indices]\n",
        "\n",
        "print(f\"Training count: {len(train_x)}, Validation count: \" f\"{len(val_x)}, Test count: {len(test_x)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_x)"
      ],
      "metadata": {
        "id": "W69FSPq1gNlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhVNyKSiNmLK"
      },
      "source": [
        "## Define MONAI transforms, Dataset and Dataloader to pre-process data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJHzJePoNmLK"
      },
      "outputs": [],
      "source": [
        "train_transforms = Compose(\n",
        "    [\n",
        "        LoadImage(image_only=True),\n",
        "        EnsureChannelFirst(),\n",
        "        ScaleIntensity(),\n",
        "        RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),\n",
        "        RandFlip(spatial_axis=0, prob=0.5),\n",
        "        RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transforms = Compose([LoadImage(image_only=True), EnsureChannelFirst(), ScaleIntensity()])\n",
        "\n",
        "y_pred_trans = Compose([Activations(softmax=True)])\n",
        "y_trans = Compose([AsDiscrete(to_onehot=num_class)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJFQOKzeNmLK"
      },
      "outputs": [],
      "source": [
        "class MedNISTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_files, labels, transforms):\n",
        "        self.image_files = image_files\n",
        "        self.labels = labels\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.transforms(self.image_files[index]), self.labels[index], self.image_files[index]\n",
        "\n",
        "\n",
        "train_ds = MedNISTDataset(train_x, train_y, train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=300, shuffle=True, num_workers=10)\n",
        "\n",
        "val_ds = MedNISTDataset(val_x, val_y, val_transforms)\n",
        "val_loader = DataLoader(val_ds, batch_size=300, num_workers=10)\n",
        "\n",
        "test_ds = MedNISTDataset(test_x, test_y, val_transforms)\n",
        "test_loader = DataLoader(test_ds, batch_size=300, num_workers=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1JynE3PNmLK"
      },
      "source": [
        "## Define network and optimizer\n",
        "\n",
        "1. Set learning rate for how much the model is updated per batch.\n",
        "1. Set total epoch number, as we have shuffle and random transforms, so the training data of every epoch is different.  \n",
        "And as this is just a get start tutorial, let's just train 4 epochs.  \n",
        "If train 10 epochs, the model can achieve 100% accuracy on test dataset.\n",
        "1. Use DenseNet from MONAI and move to GPU devide, this DenseNet can support both 2D and 3D classification tasks.\n",
        "1. Use Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnRuqLa-NmLL"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=num_class).to(device)\n",
        "# model = ResNet(spatial_dims=2, in_channels=1, out_channels=num_class).to(device)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n",
        "max_epochs = 4\n",
        "val_interval = 1\n",
        "auc_metric = ROCAUCMetric()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNQMAW5aNmLL"
      },
      "source": [
        "## Model training\n",
        "\n",
        "Execute a typical PyTorch training that run epoch loop and step loop, and do validation after every epoch.  \n",
        "Will save the model weights to file if got best validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "GnZq8RGHNmLL"
      },
      "outputs": [],
      "source": [
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    for batch_data in train_loader:\n",
        "        step += 1\n",
        "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        print(f\"{step}/{len(train_ds) // train_loader.batch_size}, \" f\"train_loss: {loss.item():.4f}\")\n",
        "        epoch_len = len(train_ds) // train_loader.batch_size\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    if (epoch + 1) % val_interval == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
        "            y = torch.tensor([], dtype=torch.long, device=device)\n",
        "            for val_data in val_loader:\n",
        "                val_images, val_labels = (\n",
        "                    val_data[0].to(device),\n",
        "                    val_data[1].to(device),\n",
        "                )\n",
        "                y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
        "                y = torch.cat([y, val_labels], dim=0)\n",
        "            y_onehot = [y_trans(i) for i in decollate_batch(y, detach=False)]\n",
        "            y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n",
        "            auc_metric(y_pred_act, y_onehot)\n",
        "            result = auc_metric.aggregate()\n",
        "            auc_metric.reset()\n",
        "            del y_pred_act, y_onehot\n",
        "            metric_values.append(result)\n",
        "            acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
        "            acc_metric = acc_value.sum().item() / len(acc_value)\n",
        "            if result > best_metric:\n",
        "                best_metric = result\n",
        "                best_metric_epoch = epoch + 1\n",
        "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model.pth\"))\n",
        "                print(\"saved new best metric model\")\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current AUC: {result:.4f}\"\n",
        "                f\" current accuracy: {acc_metric:.4f}\"\n",
        "                f\" best AUC: {best_metric:.4f}\"\n",
        "                f\" at epoch: {best_metric_epoch}\"\n",
        "            )\n",
        "\n",
        "print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU2-zMZkNmLL"
      },
      "source": [
        "## Plot the loss and metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCt5WeGbNmLL"
      },
      "outputs": [],
      "source": [
        "plt.figure(\"train\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Epoch Average Loss\")\n",
        "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
        "y = epoch_loss_values\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Val AUC\")\n",
        "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
        "y = metric_values\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHVssie5NmLL"
      },
      "source": [
        "## Evaluate the model on test dataset\n",
        "\n",
        "After training and validation, we already got the best model on validation test.  \n",
        "We need to evaluate the model on test dataset to check whether it's robust and not over-fitting.  \n",
        "We'll use these predictions to generate a classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI_bJo4fNmLL"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "  for test_data in test_loader:\n",
        "        test_images, test_labels = (\n",
        "            test_data[0].to(device),\n",
        "            test_data[1].to(device),\n",
        "        )\n",
        "        pred = model(test_images).argmax(dim=1)\n",
        "        for i in range(len(pred)):\n",
        "            y_true.append(test_labels[i].item())\n",
        "            y_pred.append(pred[i].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "DnIZYDrGNmLL"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Experiment Data on GDrive"
      ],
      "metadata": {
        "id": "kOC1xmM6Wrv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = 'MedNIST'\n",
        "model_name = \"DenseNet121\"\n",
        "\n",
        "param_str = \"\"\n",
        "# param_str = \"growth_rate:16\""
      ],
      "metadata": {
        "id": "GSyJZy3uKFT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_save_root = save_root / dataset_name / model_name"
      ],
      "metadata": {
        "id": "hWubGHqQDfQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save trained model"
      ],
      "metadata": {
        "id": "VMlMX5QrC78r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Define the source and destination paths\n",
        "src_path = str(os.path.join(root_dir, \"best_metric_model.pth\"))\n",
        "\n",
        "dst_path = experiment_save_root / f\"best_metric_model({param_str}).pth\"\n",
        "dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(src_path)\n",
        "print(dst_path)\n",
        "# Copy the file to the destination\n",
        "shutil.copy(src_path, str(dst_path))"
      ],
      "metadata": {
        "id": "kmCF7QlkWzol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "save data loaders\n",
        "\n",
        "\n",
        "this preserves the datasplits"
      ],
      "metadata": {
        "id": "z8q2OriPC9p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the data loaders to pickle files\n",
        "with open(experiment_save_root / 'train_loader.pkl', 'wb') as f:\n",
        "    pickle.dump(train_loader, f)\n",
        "\n",
        "with open(experiment_save_root / 'val_loader.pkl', 'wb') as f:\n",
        "    pickle.dump(val_loader, f)\n",
        "\n",
        "with open(experiment_save_root / 'test_loader.pkl', 'wb') as f:\n",
        "    pickle.dump(test_loader, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "DZ1uJCQzC3sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CAManim"
      ],
      "metadata": {
        "id": "olwGKpBHWHyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment setup"
      ],
      "metadata": {
        "id": "IeXTvHOTWMxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "iz8Xy-XFMrcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -version"
      ],
      "metadata": {
        "id": "L4qytLcnWjb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memory_profiler\n",
        "%load_ext memory_profiler"
      ],
      "metadata": {
        "id": "AA6Q1xxeMtKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install CAManim repo"
      ],
      "metadata": {
        "id": "vu3gKYW-MxdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/OMNI-ML/pytorch-grad-cam-anim.git\n",
        "!ls\n",
        "\n",
        "# the % command makes the cd last beyond this cell / line\n",
        "%cd pytorch-grad-cam-anim\n",
        "\n",
        "# switch branch\n",
        "!git checkout adapt-basecam-to-support-cam_anim\n",
        "!ls"
      ],
      "metadata": {
        "id": "0FbJsETfeiIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git log --oneline --max-count=10 # --reverse"
      ],
      "metadata": {
        "id": "R-An-q6pWT67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python setup.py install - avoid this\n",
        "!pip install .\n",
        "!pip install -r requirements_CAManim.txt\n",
        "# !pip install ffmpeg-python"
      ],
      "metadata": {
        "id": "_n6GA5jUWRsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "T7UK1fZJNBju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import pickle\n",
        "import torch"
      ],
      "metadata": {
        "id": "E5TnYCBucMcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define CAMs list"
      ],
      "metadata": {
        "id": "3VrieyM6Zdno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, EigenGradCAM, AblationCAM, RandomCAM\n",
        "from pytorch_grad_cam import EigenCAM, \\\n",
        "                              EigenGradCAM, \\\n",
        "                              FullGrad, \\\n",
        "                              GradCAM, \\\n",
        "                              GradCAMElementWise, \\\n",
        "                              GradCAMPlusPlus, \\\n",
        "                              HiResCAM, \\\n",
        "                              LayerCAM, \\\n",
        "                              RandomCAM, \\\n",
        "                              ScoreCAM, \\\n",
        "                              XGradCAM, \\\n",
        "                              AblationCAM\n",
        "\n",
        "\n",
        "cams_list = [#'AblationCAM', # ran out of RAM\n",
        "            'GradCAM',\n",
        "            'HiResCAM',\n",
        "            'GradCAMElementWise',\n",
        "            'GradCAMPlusPlus',\n",
        "            'EigenCAM', # done\n",
        "            #'EigenGradCAM', # Skipped all layers for AlexNet; figure out\n",
        "            'RandomCAM',\n",
        "            'LayerCAM',\n",
        "             'XGradCAM',\n",
        "             #'FullGrad', # Skipped all layers for DenseNet161; figure out\n",
        "             #'ScoreCAM', # ran out of RAM\n",
        "             ]"
      ],
      "metadata": {
        "id": "y9Pp1kMSZhxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cams_list = ['AblationCAM']#['HiResCAM']#['GradCAM']"
      ],
      "metadata": {
        "id": "jUcre3iibJ_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Image"
      ],
      "metadata": {
        "id": "evu_9h_7ZdVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MedNISTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_files, labels, transforms):\n",
        "        self.image_files = image_files\n",
        "        self.labels = labels\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.transforms(self.image_files[index]), self.labels[index], self.image_files[index]"
      ],
      "metadata": {
        "id": "fBrwF0bC85MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data loaders from the pickle files\n",
        "with open(experiment_save_root / 'train_loader.pkl', 'rb') as f:\n",
        "    train_loader = pickle.load(f)\n",
        "\n",
        "with open(experiment_save_root / 'val_loader.pkl', 'rb') as f:\n",
        "    val_loader = pickle.load(f)\n",
        "\n",
        "with open(experiment_save_root / 'test_loader.pkl', 'rb') as f:\n",
        "    test_loader = pickle.load(f)"
      ],
      "metadata": {
        "id": "p03sPIBQD2tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "#with torch.no_grad():\n",
        "found = False\n",
        "for test_data in test_loader:\n",
        "    test_images, test_labels, test_paths = (\n",
        "        test_data[0].to(device),\n",
        "        test_data[1].to(device),\n",
        "        test_data[2]\n",
        "    )\n",
        "\n",
        "    for img_id, img_path in enumerate(test_paths):\n",
        "\n",
        "      # if \"/content/MedNIST/Hand/008908.jpeg\" == img_path:\n",
        "      if \"Hand\" in img_path:# and \"008908\" in img_path:\n",
        "        found = True\n",
        "        break\n",
        "      # if \"Hand\" in img_path:\n",
        "      #   found = True\n",
        "      #   break\n",
        "\n",
        "    if found:\n",
        "      break\n",
        "\n",
        "\n",
        "# img_id = 2\n",
        "\n",
        "# \"/content/MedNIST/Hand/008908.jpeg\"\n",
        "\n",
        "#target_layers = get_target_layers(model)\n",
        "input_tensor = torch.unsqueeze(test_images[img_id], 0)\n",
        "input_tensor.requires_grad_(True)\n",
        "img = np.array(PIL.Image.open(test_paths[img_id]))\n",
        "img = np.expand_dims((np.float32(img) / 255), axis=2)\n",
        "print(img.shape)\n",
        "print(test_paths[img_id])"
      ],
      "metadata": {
        "id": "whNZgZ7Ubv7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Models Dictionary"
      ],
      "metadata": {
        "id": "Gymbn-2-Z5S1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = 'MedNIST'\n",
        "model_name = \"DenseNet121\"\n",
        "param_str = \"\"\n",
        "# param_str = \"growth_rate:16\"\n",
        "\n",
        "# experiment_save_root = save_root / dataset_name / model_name\n",
        "model_path = str(save_root / dataset_name / model_name / f\"best_metric_model({param_str}).pth\")\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "# model_name = 'MedNet'\n",
        "\n",
        "models_dict = {\n",
        "                f\"{dataset_name}_{model_name}({param_str})\": model\n",
        "              }"
      ],
      "metadata": {
        "id": "yFhKPir_Z9il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### visualize model parameters"
      ],
      "metadata": {
        "id": "sAN0sHP2a9iH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_key = list(models_dict.keys())[0] # pick first model in dictionary\n",
        "print(model_key)\n",
        "\n",
        "model = models_dict[model_key]\n",
        "layers = [layer for layer, what_is_this in model.named_modules()]\n",
        "print(len(layers))\n",
        "parameters = [p for p in model.parameters()]# if p.requires_grad]\n",
        "len(parameters)"
      ],
      "metadata": {
        "id": "4zZ8eZ2xa83u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "running_sum = 0\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "for layer_name, layer_module in model.named_modules():\n",
        "\n",
        "  # print(\"-\"*6 + layer_name + \"-\"*6)\n",
        "  # print(type(layer_name))\n",
        "  # print(type(layer_module))\n",
        "  # layer_parameters = [parameters = [p for p in model.parameters()]# if p.requires_grad]]\n",
        "  n_layer_params = sum(p.numel() for p in layer_module.parameters() if p.requires_grad)\n",
        "  running_sum += n_layer_params\n",
        "\n",
        "print(running_sum)\n",
        "print(total_params)"
      ],
      "metadata": {
        "id": "hEpwPgZra95Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CAManim Loop"
      ],
      "metadata": {
        "id": "r_0az7sdZq3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### utils"
      ],
      "metadata": {
        "id": "dCBAbOkabZIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.enable()\n",
        "print(gc.collect())\n",
        "print(pd.DataFrame.from_records(gc.get_stats()))"
      ],
      "metadata": {
        "id": "1QN6CgJhbrKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target_layers(model):\n",
        "  \"\"\"\n",
        "  target_layers doesn't matter for cam_anim (since we loop through all the layers),\n",
        "  but it is needed for initializing the GradCAM object.\n",
        "  This method returns the last layer (that is viable as a target i.e. iterable) of the model.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  target_layers = []\n",
        "  for _, layer_module in model.named_modules():\n",
        "    # print(_)\n",
        "    if _ == \"features.Mixed_7c.branch_pool.conv\":\n",
        "      target_layers.append(layer_module)\n",
        "    # https://stackoverflow.com/questions/1952464/in-python-how-do-i-determine-if-an-object-is-iterable\n",
        "    try:\n",
        "      some_object_iterator = iter(layer_module)\n",
        "      if type(layer_module)!=str:\n",
        "        target_layers.append(layer_module)\n",
        "\n",
        "    except TypeError as te:\n",
        "      pass\n",
        "      # print(some_object, 'is not iterable')\n",
        "  # target_layers = target_layers[-2]\n",
        "  print(target_layers[-1])\n",
        "  return [target_layers[-1]]"
      ],
      "metadata": {
        "id": "uQE4aSPebgJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loop"
      ],
      "metadata": {
        "id": "8VePZH6jbczG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_models = len(models_dict.keys())\n",
        "n_cams = len(cams_list)\n",
        "\n",
        "print(f\"{n_models} X {n_cams} = {n_models*n_cams} iterations\")"
      ],
      "metadata": {
        "id": "EkMDmhRjqVaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_records = []\n",
        "for model_name, model in models_dict.items():\n",
        "\n",
        "  print(\"=\"*33 + model_name + \"=\"*33)\n",
        "\n",
        "  outdir = save_root / model_name\n",
        "  outdir.mkdir(exist_ok=True, parents= True)\n",
        "\n",
        "  target_layers = get_target_layers(model)\n",
        "  # target_layers = [\"features.Mixed_7c.branch_pool.conv\"]\n",
        "\n",
        "  for cam_name in cams_list:\n",
        "    print(\"-\"*33 + cam_name + \"-\"*33)\n",
        "\n",
        "    frames_dir = outdir / cam_name\n",
        "    frames_dir.mkdir(exist_ok=True, parents= True)\n",
        "    metrics = {\"model_name\": model_name, \"CAM Name\": cam_name, \"error\": None}\n",
        "    try:\n",
        "      cam = globals()[cam_name](model=model, target_layers=target_layers, use_cuda=False)\n",
        "      metrics_update = cam.cam_anim(img,\n",
        "                                    input_tensor,\n",
        "                                    frame_rate=24,\n",
        "                                    norm_type='both',\n",
        "                                    keep_frames=True,\n",
        "                                    tmp_dir=str(frames_dir),\n",
        "                                    output_fname=str(outdir / f\"{cam_name}_anim.mp4\"))\n",
        "\n",
        "      metrics.update(metrics_update)\n",
        "      del cam\n",
        "      gc.collect()\n",
        "      print(pd.DataFrame.from_records(gc.get_stats()))\n",
        "      print(f\"{cam_name} Finished\")\n",
        "\n",
        "\n",
        "    except Exception as ex:\n",
        "      metrics.update({\"error\": str(ex)})\n",
        "      print(ex)\n",
        "      print(\"Continuing to the next CAM\")\n",
        "      raise ex\n",
        "\n",
        "    metrics_records.append(metrics)\n",
        "\n",
        "    with open(str(outdir / f\"metrics_logs.json\"), \"w\") as outfile:\n",
        "      json.dump(metrics_records, outfile, indent=4)"
      ],
      "metadata": {
        "id": "z8pkH_yccTdi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bwyUGwWfWUOi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}