# -*- coding: utf-8 -*-
"""*CAManim_animating-end2end-activation-maps.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EMIcSV4JQdM2-H3RiEG0zUo08fOl6TuL

# Animating End-to-End Network Actication Maps

Authors: Emily Kaczmarek, Olivier Miguel, Kevin Dick

---

#### Resources:
* GitHub repo to push our changes to: https://github.com/jacobgil/pytorch-grad-cam
* Command for ffmpeg: `ffmpeg -framerate 5 -pattern_type glob -i "*.png" -c:v libx264 -r 30 -vf "scale=1200:-2,format=yuv420p" -movflags +faststart output.mp4`

TODO: write fancy things here

#### Hackathon Day 1:
* Determine a few example benchmark image datasets that we demo
  * MNIST
  * whatever GradCAM paper used
  * Anotherrrrr
* Determine what pre-trained model architecture we use:
 * DenseNet (no isses in layer name
"""

# !pip install grad-cam

# ! pip install ffmpeg-python

import os
import glob
import shutil
import ffmpeg
import numpy as np
import matplotlib.pyplot as plt
from operator import attrgetter
from pytorch_grad_cam.utils.image import show_cam_on_image

import torch
import ttach as tta
from typing import Callable, List, Tuple
from pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients
from pytorch_grad_cam.utils.svd_on_activations import get_2d_projection
from pytorch_grad_cam.utils.image import scale_cam_image
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget

from pytorch_grad_cam.base_cam import BaseCAM

class CAM_Anim(BaseCAM):
    def __init__(self, model, target_layers, use_cuda=False,
                 reshape_transform=None):
        super(
            HiResCAM,
            self).__init__(
            model,
            target_layers,
            use_cuda,
            reshape_transform)

    def cam_anim(model, 
             architecture_type, 
             img,
             img_tensor,
             cam_func_name='GradCAM',
             norm_type='global', # TODO: implement "both" 
             targets=None,
             frame_rate=5, 
             tmp_dir='tmp_anim', 
             output_fname='output.mp4',
             keep_frames=False,
             overlay=True, # TODO: implement this
             return_layer_name_map=False,
             use_cuda=False,
             quality='standard'):
        """_summary_

        Args:
            model (_type_): _description_
            architecture_type (_type_): _description_
            img (_type_): _description_
            img_tensor (_type_): _description_
            cam_func_name (str, optional): the exact name of the CAM class to use; options: ['GradCAM', 'HiResCAM', ...]. Defaults to 'GradCAM'.
            norm_type (str, optional): _description_. Defaults to 'global'.
            frame_rate (int, optional): _description_. Defaults to 5.
            tmp_dir (str, optional): _description_. Defaults to 'tmp_anim'.
            output_fname (str, optional): _description_. Defaults to 'output.mp4'.
            keep_frames (bool, optional): _description_. Defaults to False.
            overlay (bool, optional): _description_. Defaults to True.
            use_cuda (bool, optional): _description_. Defaults to False.
            quality (str, optional): _description_. Defaults to 'standard'.

        Returns:
            _type_: _description_
        """
      """ cam_anim
      TODO: General description here
      --- general parameters
      model <PyTorch.nn>, the pretrained model used to generate layer-wise activations
      architecture_type <str>, the type of model architecture;
        options: ['densenet', 'resnet', 'vgg', ...]
      cam_func_name <str>, the exact name of the CAM class to use;
        options: ['GradCAM', 'HiResCAM', ...]
      global_layer_norm=True, normalize accross all layers for smooth transition;
        if False, we perform layer-spefic normalization
      include_final_map=True <bool>, optionally includes the final layer map
      return_layer_name_map=False <bool>, optionally returns a list of the layer name map (network vs. ordered frame) 
      use_cuda=True <bool>, TODO

      --- storage-specific parameters
      tmp_dir='tmp_anim' <str, Path>, os.path.join(os.getcwd(), 'tmp_anim')
      output_fname='output.mp4' <str, Path>, path to where the output file is saved
      keep_frames=False <bool>, determines whether or not the interim activation frames are retained or deleted
      --- ffmpeg-specific parameters
      frame_rate=5,

      --- animation-specific parameters
      quality='standard' <str>, determines what level of quality to render the video via ffmpeg: ['standard', 'high']
      include_title=True <bool>, adds the layer name to each picture title
      colormap='jet'

  """

  # cast the tmp_dir to string-representation (if passed as a Path object) TODO: test this with PATHS
  if not tmp_dir=='tmp_anim': tmp_dir = str(tmp_dir)
  
  # ensure that the path ends with a terminal os.sep (OS-agnostic)
  if not tmp_dir.endswith(os.sep): tmp_dir += os.sep
  
  # create the tmp_dir where images will be stored (clean out if already exists)
  if not os.path.exists(tmp_dir):
    try:
      os.mkdir(tmp_dir)
    except: 
      print('ERROR: failed to create tmp_dir: ' + tmp_dir + '. Exitting.')
      return None
  else: # clean out any existing images if the dir already exists
    for f in glob.glob(tmp_dir + '*'): os.remove(f)

  # EMILY ------------------------------------  
  # Generate & save images/arrays for all layers; save them to tmp_dir

  count = 0
  layers = []
  layer_name_map = {}
  temp_dict = {}

  for layer, _ in model.named_modules(): layers.append(layer)
  for target_layer in layers:
    try: 
      cam = globals()[cam_func_name](model=model, 
                     target_layers=[attrgetter(target_layer)(model)],
                     use_cuda=use_cuda,
                     normalization=False)
      temp_dict[str("%06d"%count)] = cam(input_tensor=img_tensor, targets=targets)
      layer_name_map[str("%06d"%count)] = target_layer
      count += 1
    except:
      # TODO: add more informative thing here
      print('skipping ' + target_layer)
      continue
    
  mx = np.max(np.concatenate(list(temp_dict.values())))
  mn = np.min(np.concatenate(list(temp_dict.values())))

  for key, val in temp_dict.items():
    if norm_type == 'global' or norm_type == 'both':
      val = (val - mn) / (mx - mn)
      create_image_as_png(img, key, val, layer_name_map, tmp_dir+'global')
    elif norm_type == 'layer' or norm_type == 'both':
      val = (val - np.min(val)) / (np.max(val) - np.min(val))
      create_image_as_png(img, key, val, layer_name_map, tmp_dir+'layer')

  # O(L*nm), n=width, m=height
  # EMILY ------------------------------------
 
  # if the output_fname already exists, ffmpeg will fail, so we overwrite it by deleting original.
  if os.path.exists(output_fname): os.remove(output_fname)

  # generate the animation; automatically saves to file
  if quality =='high': _ffmpeg_high_quality(tmp_dir, output_fname, frame_rate=frame_rate)
  else: _ffmpeg_standard_quality(tmp_dir, output_fname, frame_rate=frame_rate)

  # if we do not keep the individual frames, we remove the tmp_dir & contents
  if not keep_frames: shutil.rmtree(tmp_dir)

  # return the list of two-tuples mapping original layer name to new filename
  if return_layer_name_map:
    return layer_name_map

  # done!
  return None